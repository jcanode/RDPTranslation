{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit923bdb9dbde4441fa27ccb5ba6d2aec9",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all requirments\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import time\n",
    "import  os\n",
    "import io\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time required to load data --- 0.0 seconds ---\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "## load data\n",
    "# german_data_path = os.path.join(os.path.curdir,\"./data/europarl-v7.de-en.de\")\n",
    "german_data_path= r\"C:\\Users\\justi\\Documents\\RDPTranslation\\initalTest\\data\\europarl-v7.de-en.de\"\n",
    "# english_data_path = os.path.join(os.path.curdir,\"./data/europarl-v7.de-en.en\")\n",
    "english_data_path = r\"C:\\Users\\justi\\Documents\\RDPTranslation\\initalTest\\data\\europarl-v7.de-en.en\"\n",
    "print(\"Time required to load data --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time required to create dataset --- 4.76974630355835 seconds ---\n"
    }
   ],
   "source": [
    "## Load data from file and into a dataset in memory\n",
    "start_time = time.time()\n",
    "def create_dataset(germ_path, eng_path):\n",
    "  germ_lines = io.open(germ_path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  eng_lines = io.open(eng_path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  return germ_lines, eng_lines\n",
    "german_data, english_data = create_dataset(german_data_path, english_data_path)\n",
    "print(\"Time required to create dataset --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Read english tokenizer input from  C:\\Users\\justi\\Documents\\RDPTranslation\\initalTest\\tokenizerConfig\\english_config.json\nRead german tokenizer input from  C:\\Users\\justi\\Documents\\RDPTranslation\\initalTest\\tokenizerConfig\\english_config.json\nTime required to tokenize data --- 96.34556245803833 seconds ---\n"
    }
   ],
   "source": [
    "## Tokenize german and english sequences\n",
    "start_time = time.time()\n",
    "\n",
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "## Uncomment these lines for first time running the program to configure tokenizer (helps speed up run time)\n",
    "# en_tokenizer.fit_on_texts(english_data)\n",
    "# english_config = en_tokenizer.to_json()\n",
    "\n",
    "# english_config_path = os.path.join(r\"./tokenizerConfig/english_config.json\")\n",
    "\n",
    "english_config_path = r\"C:\\Users\\justi\\Documents\\RDPTranslation\\initalTest\\tokenizerConfig\\english_config.json\"\n",
    "## Uncomment these lines for first time running the program to configure tokenizer (helps speed up run time)\n",
    "# io.open(english_config_path,'w').write(english_config)\n",
    "# print(\"Wrote english config to \", english_config_path)\n",
    "\n",
    "## Comment lines out first time runnint to avoid reloading config\n",
    "english_config = io.open(english_config_path, encoding='UTF-8').read()\n",
    "en_tokenizer =  tf.keras.preprocessing.text.tokenizer_from_json(english_config)\n",
    "print(\"Read english tokenizer input from \", english_config_path)\n",
    "\n",
    "## Tokenize english data\n",
    "data_en = en_tokenizer.texts_to_sequences(english_data)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding='post')\n",
    "\n",
    "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "## Uncomment these lines for first time running the program to configure tokenizer (helps speed up run time)\n",
    "# ge_tokenizer.fit_on_texts(german_data)\n",
    "# german_config = ge_tokenizer.to_json()\n",
    "\n",
    "# german_config_path = os.path.join(r\"./tokenizerConfig/german_config.json\")\n",
    "german_config_path = r\"C:\\Users\\justi\\Documents\\RDPTranslation\\initalTest\\tokenizerConfig\\english_config.json\"\n",
    "## Uncomment these lines for first time running the program to configure tokenizer (helps speed up run time)\n",
    "# io.open(german_config_path,'w').write(german_config)\n",
    "# print(\"Wrote german config to \", german_config_path)\n",
    "\n",
    "## Comment lines out first time runnint to avoid reloading config\n",
    "german_config = io.open(german_config_path, encoding='UTF-8').read()\n",
    "ge_tokenizer =  tf.keras.preprocessing.text.tokenizer_from_json(german_config)\n",
    "print(\"Read german tokenizer input from \", german_config_path)\n",
    "\n",
    "## tokenize german data\n",
    "data_ge = ge_tokenizer.texts_to_sequences(german_data)\n",
    "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding='post',maxlen=668)\n",
    "\n",
    "print(\"Time required to tokenize data --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5130798448 bytes\n"
    }
   ],
   "source": [
    "print(\"%d bytes\" % (data_en.size * data_en.itemsize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time required to slice data --- 4.167998790740967 seconds ---\n"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "## slice data with an 80/20 split\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_ge,test_size=0.4)\n",
    "print(\"Time required to slice data --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1920209, 668)\n"
    }
   ],
   "source": [
    "print(data_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1920209, 668, 1)\n(1920209, 668, 1)\n"
    }
   ],
   "source": [
    "## Convert data to tensor and print shape of X_train (english training data)\n",
    "Dtype = tf.float32\n",
    "\n",
    "X_train = tf.convert_to_tensor(data_en,dtype=Dtype)\n",
    "X_train = tf.reshape(X_train, [X_train.shape[0], X_train.shape[1], 1])\n",
    "print(X_train.shape)\n",
    "X_test = tf.convert_to_tensor(X_test,dtype=Dtype)\n",
    "Y_train = tf.convert_to_tensor(data_ge,dtype=Dtype)\n",
    "Y_train = tf.reshape(Y_train, [Y_train.shape[0], Y_train.shape[1], 1])\n",
    "print(Y_train.shape)\n",
    "Y_test = tf.convert_to_tensor(Y_test,dtype=Dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "set hyperparms\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Define model (cnn seq2seq)\n",
    "\n",
    "#Hyperparams            (from https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 128 #len(X_train)\n",
    "# steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dims = 256\n",
    "rnn_units = 1024\n",
    "dense_units = 1024\n",
    "num_batches = len(X_train)/BATCH_SIZE\n",
    "Dtype = tf.float32\n",
    "print(\"set hyperparms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 668, 1)\n(64, 668, 1)\n"
    }
   ],
   "source": [
    "\n",
    "# input_vocab_size = len(en_tokenizer.word_index)+1\n",
    "# output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "example_X, example_Y = next(iter(dataset))\n",
    "example_X = tf.reshape(example_X,[example_X.shape[0],example_X.shape[1],1])\n",
    "example_Y = tf.reshape(example_Y,[example_Y.shape[0],example_Y.shape[1],1])\n",
    "\n",
    "# example_X.set_shape([example_X.shape[0],example_X.shape[1],1])\n",
    "print(example_X.shape)\n",
    "print(example_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "18001\n"
    }
   ],
   "source": [
    "num_batches = int(len(X_train)/BATCH_SIZE)\n",
    "print(num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    model = tf.keras.Sequential(layers=[\n",
    "        # tf.keras.layers.LSTM(64),\n",
    "        # tf.keras.layers.LSTM(16),\n",
    "        tf.keras.layers.Conv1D(668,kernel_size=1),\n",
    "        tf.keras.layers.Conv1D(128,kernel_size=1),\n",
    "        tf.keras.layers.Conv1D(64,kernel_size=1),\n",
    "        tf.keras.layers.Conv1D(1,kernel_size=1),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # tf.keras.layers.Conv1D(64,kernel_size=1), #input_shape=(64,668)\n",
    "        # tf.keras.layers.MaxPool1D(),\n",
    "        # tf.keras.layers.Conv1D(16,kernel_size=1),\n",
    "        # tf.keras.layers.Dense(16),\n",
    "        # # # tf.keras.layers.MaxPool1D(),\n",
    "        # #\n",
    "        tf.keras.layers.Dense(16),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dense(668),\n",
    "\n",
    "        # tf.keras.layers.Conv1DTranspose(1,kernel_size=1),\n",
    "        # tf.keras.layers.Conv1DTranspose(668,kernel_size=1)\n",
    "\n",
    "        ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel2():\n",
    "    model = tf.keras.Sequential(layers=[\n",
    "        # tf.keras.layers.LSTM(64),\n",
    "        # tf.keras.layers.LSTM(16),\n",
    "        # tf.keras.layers.Conv1D(668,kernel_size=1),\n",
    "        # tf.keras.layers.Conv1D(128,kernel_size=1),\n",
    "        # tf.keras.layers.Conv1D(64,kernel_size=1),\n",
    "        # tf.keras.layers.Conv1D(1,kernel_size=1),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # tf.keras.layers.Conv1D(64,kernel_size=1), #input_shape=(64,668)\n",
    "        # tf.keras.layers.MaxPool1D(),\n",
    "        # tf.keras.layers.Conv1D(16,kernel_size=1),\n",
    "        # tf.keras.layers.Dense(16),\n",
    "        # # # tf.keras.layers.MaxPool1D(),\n",
    "        # #\n",
    "        tf.keras.layers.Dense(668),\n",
    "        tf.keras.layers.Dense(668),\n",
    "        tf.keras.layers.Dense(668),\n",
    "        tf.keras.layers.Dense(668),\n",
    "        tf.keras.layers.Dense(668),\n",
    "        tf.keras.layers.Dense(668),\n",
    "        # tf.keras.layers.Conv1DTranspose(1,kernel_size=1),\n",
    "        # tf.keras.layers.Conv1DTranspose(668,kernel_size=1)\n",
    "\n",
    "        ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'buildModel2' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f1b51a69446c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdensemodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildModel2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time required to create model --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# seq2seq.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'buildModel2' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "densemodel = buildModel2()\n",
    "print(\"Time required to create model --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# seq2seq.summary()\n",
    "\n",
    "## Train model and print results\n",
    "densemodel.compile(optimizer= 'adam',loss=\"CategoricalCrossentropy\")\n",
    "# seq2seq.build(input_shape=(668,64))\n",
    "# seq2seq.summary()\n",
    "densemodel.fit(x=example_X,y=example_Y,batch_size=BATCH_SIZE,epochs=600)\n",
    "densemodel.summary()\n",
    "densemodel.save(filepath=\"./models/dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(768084, 668, 1)\n(768084, 668, 1)\n"
    }
   ],
   "source": [
    "X_test = tf.reshape(X_test,[X_test.shape[0],X_test.shape[1],1])\n",
    "Y_test = tf.reshape(Y_test,[Y_test.shape[0],Y_test.shape[1],1])\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Time required to create model --- 0.030010223388671875 seconds ---\nEpoch 1/100\n1/1 [==============================] - 0s 3ms/step - loss: nan\nEpoch 2/100\n1/1 [==============================] - 0s 3ms/step - loss: nan\nEpoch 3/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 4/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 5/100\n1/1 [==============================] - 0s 998us/step - loss: nan\nEpoch 6/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 7/100\n1/1 [==============================] - 0s 3ms/step - loss: nan\nEpoch 8/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 9/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 10/100\n\nEpoch 00010: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 11/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 12/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 13/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 14/100\n1/1 [==============================] - 0s 995us/step - loss: nan\nEpoch 15/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 16/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 17/100\n1/1 [==============================] - 0s 994us/step - loss: nan\nEpoch 18/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 19/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 20/100\n\nEpoch 00020: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 21/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 22/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 23/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 24/100\n1/1 [==============================] - 0s 998us/step - loss: nan\nEpoch 25/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 26/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 27/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 28/100\n1/1 [==============================] - 0s 995us/step - loss: nan\nEpoch 29/100\n1/1 [==============================] - 0s 998us/step - loss: nan\nEpoch 30/100\n\nEpoch 00030: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 31/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 32/100\n1/1 [==============================] - 0s 3ms/step - loss: nan\nEpoch 33/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 34/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 35/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 36/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 37/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 38/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 39/100\n1/1 [==============================] - 0s 997us/step - loss: nan\nEpoch 40/100\n\nEpoch 00040: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 41/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 42/100\n1/1 [==============================] - 0s 999us/step - loss: nan\nEpoch 43/100\n1/1 [==============================] - 0s 1000us/step - loss: nan\nEpoch 44/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 45/100\n1/1 [==============================] - 0s 995us/step - loss: nan\nEpoch 46/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 47/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 48/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 49/100\n1/1 [==============================] - 0s 998us/step - loss: nan\nEpoch 50/100\n\nEpoch 00050: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 51/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 52/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 53/100\n1/1 [==============================] - 0s 996us/step - loss: nan\nEpoch 54/100\n1/1 [==============================] - 0s 997us/step - loss: nan\nEpoch 55/100\n1/1 [==============================] - 0s 986us/step - loss: nan\nEpoch 56/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 57/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 58/100\n1/1 [==============================] - 0s 997us/step - loss: nan\nEpoch 59/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 60/100\n\nEpoch 00060: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 61/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 62/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 63/100\n1/1 [==============================] - 0s 999us/step - loss: nan\nEpoch 64/100\n1/1 [==============================] - 0s 998us/step - loss: nan\nEpoch 65/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 66/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 67/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 68/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 69/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 70/100\n\nEpoch 00070: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 994us/step - loss: nan\nEpoch 71/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 72/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 73/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 74/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 75/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 76/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 77/100\n1/1 [==============================] - 0s 3ms/step - loss: nan\nEpoch 78/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 79/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 80/100\n\nEpoch 00080: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 81/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 82/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 83/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 84/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 85/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 86/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 87/100\n1/1 [==============================] - 0s 994us/step - loss: nan\nEpoch 88/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 89/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 90/100\n\nEpoch 00090: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 91/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 92/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 93/100\n1/1 [==============================] - 0s 999us/step - loss: nan\nEpoch 94/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 95/100\n1/1 [==============================] - 0s 1ms/step - loss: nan\nEpoch 96/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 97/100\n1/1 [==============================] - 0s 998us/step - loss: nan\nEpoch 98/100\n1/1 [==============================] - 0s 999us/step - loss: nan\nEpoch 99/100\n1/1 [==============================] - 0s 2ms/step - loss: nan\nEpoch 100/100\n\nEpoch 00100: saving model to models/checkpoints\\cp.ckpt\n1/1 [==============================] - 0s 1ms/step - loss: nan\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_4 (Conv1D)            (64, 668, 668)            1336      \n_________________________________________________________________\nconv1d_5 (Conv1D)            (64, 668, 128)            85632     \n_________________________________________________________________\nconv1d_6 (Conv1D)            (64, 668, 64)             8256      \n_________________________________________________________________\nconv1d_7 (Conv1D)            (64, 668, 1)              65        \n_________________________________________________________________\nflatten_1 (Flatten)          (64, 668)                 0         \n_________________________________________________________________\ndense_3 (Dense)              (64, 16)                  10704     \n_________________________________________________________________\ndense_4 (Dense)              (64, 128)                 2176      \n_________________________________________________________________\ndense_5 (Dense)              (64, 668)                 86172     \n=================================================================\nTotal params: 194,341\nTrainable params: 194,341\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "checkpoint_path = \"models/checkpoints/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,save_freq=10)\n",
    "\n",
    "seq2seq = buildModel()\n",
    "print(\"Time required to create model --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# seq2seq.summary()\n",
    "\n",
    "## Train model and print results\n",
    "seq2seq.compile(optimizer='adam',loss=\"categorical_crossentropy\")\n",
    "# seq2seq.build(input_shape=(668,64))\n",
    "# seq2seq.summary()\n",
    "seq2seq.fit(x=example_X,y=example_Y,batch_size=BATCH_SIZE,epochs=100,\n",
    "          callbacks=[cp_callback])\n",
    "seq2seq.summary()\n",
    "# seq2seq.save(filepath=\"./models/seq2seq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(768084, 668, 1)\n(768084, 668, 1)\n"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "24003/24003 [==============================] - 830s 35ms/step - loss: nan\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d358af3bc569>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restored model, accuracy: {:5.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "latest_weights = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "new_model = buildModel()\n",
    "new_model.load_weights(latest_weights)\n",
    "\n",
    "new_model.compile(optimizer='adam',loss=\"categorical_crossentropy\")\n",
    "loss, acc = new_model.evaluate(X_test, Y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X type\n<dtype: 'float32'>\n<dtype: 'float32'>\ny type\n<dtype: 'float32'>\n<dtype: 'float32'>\n"
    }
   ],
   "source": [
    "print(\"X type\")\n",
    "print(X_test.dtype)\n",
    "tf.cast(X_test,dtype=tf.int32)\n",
    "print(X_test.dtype)\n",
    "print(\"y type\")\n",
    "print(Y_test.dtype)\n",
    "tf.cast(Y_test,dtype=tf.int32)\n",
    "print(Y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<dtype: 'float32'>\n"
    }
   ],
   "source": [
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}