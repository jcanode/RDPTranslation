{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "imported all\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "print(\"imported all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "E:/RDPTranslation/initalTest/data/europarl-v7.de-en.de\n"
    }
   ],
   "source": [
    "# Download the file\n",
    "# path_to_zip = tf.keras.utils.get_file(\n",
    "#     'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "#     extract=True)\n",
    "\n",
    "# path_to_german_file = os.path.curdir+\"/../data/europol-v7.de-en.de\"\n",
    "\n",
    "# path_to_english_file = os.path.curdir+\"/../data/europol-v7.de-en.en\"\n",
    "# print(path_to_german_file)\n",
    "\n",
    "# path_to_german_file = \"C:/users/justi/Documents/RDPTranslation/initalTest/data/europarl-v7.de-en.de\"\n",
    "\n",
    "# path_to_english_file = \"C:/users/justi/Documents/RDPTranslation/initalTest/data/europarl-v7.de-en.en\"\n",
    "\n",
    "path_to_german_file = \"E:/RDPTranslation/initalTest/data/europarl-v7.de-en.de\"\n",
    "\n",
    "path_to_english_file = \"E:/RDPTranslation/initalTest/data/europarl-v7.de-en.en\"\n",
    "\n",
    "print(path_to_german_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentences(english_data, german_data):\n",
    "      germ_lines = io.open(german_data, encoding='UTF-8').read().strip().split('\\n')\n",
    "      eng_lines = io.open(english_data, encoding='UTF-8').read().strip().split('\\n')\n",
    "      german_result= []\n",
    "      english_result = []\n",
    "      combined_data = []\n",
    "      for i in range (1,30000): ## only run on first 30k lines\n",
    "        german_lines_clean = preprocess_sentence(germ_lines[i])\n",
    "        english_lines_clean = preprocess_sentence(eng_lines[i])\n",
    "        combined_data.append(english_lines_clean)\n",
    "#         combined_data.append('\\t')\n",
    "        combined_data.append(german_lines_clean)\n",
    "        german_result.append(german_lines_clean)\n",
    "        english_result.append(english_lines_clean)\n",
    "      return combined_data, german_result, english_result\n",
    "data, german_data, english_data = join_sentences(path_to_english_file,path_to_german_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "59998\n59998\n29999\n<start> i declare resumed the session of the european parliament adjourned on friday december , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . <end>\n"
    }
   ],
   "source": [
    "print(len(data))\n",
    "# data = data.strip('\\t')\n",
    "x = data[0::1]\n",
    "print(len(x))\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(x[i])\n",
    "# print(data[3])\n",
    "# print(x[3])\n",
    "# print(german_data[0])\n",
    "print(len(german_data))\n",
    "# print(german_data)\n",
    "print(english_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<start> i declare resumed the session of the european parliament adjourned on friday december , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . <end>\t<start> ich erklare die am freitag , dem . dezember unterbrochene sitzungsperiode des europaischen parlaments fur wiederaufgenommen , wunsche ihnen nochmals alles gute zum jahreswechsel und hoffe , da sie schone ferien hatten . <end>\n<start> we hope that this amendment will be adopted . <end>\n"
    }
   ],
   "source": [
    "joined_data = ['\\t'.join(x) for x in zip(data[0::2], x[1::2])]\n",
    "print(joined_data[0])\n",
    "# print(len(joined_data))\n",
    "print(english_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,dtype='float32', maxlen=179,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "#   targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  \n",
    "  input_tensor, inp_lang_tokenizer = tokenize(english_data)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(german_data)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(29999, 179)\n(29999, 179)\n"
    }
   ],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(english_data)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(german_data)\n",
    "print(input_tensor.shape)\n",
    "# print(len(inp_lang_tokenizer))\n",
    "print(target_tensor.shape)\n",
    "# print(len(targ_lang_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_english_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "23999 23999 6000 6000\n"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input Language; index to word mapping\n4 ----> <start>\n1 ----> the\n9168 ----> fvo\n11 ----> is\n110 ----> made\n88 ----> up\n6 ----> of\n765 ----> independent\n2235 ----> scientists\n8 ----> and\n6315 ----> vets\n8 ----> and\n66 ----> so\n18 ----> on\n84 ----> who\n2806 ----> travel\n44 ----> from\n89 ----> some\n3779 ----> location\n127 ----> where\n45 ----> there\n11 ----> is\n34 ----> an\n4435 ----> airport\n288 ----> quite\n283 ----> clearly\n14 ----> we\n24 ----> have\n34 ----> an\n4435 ----> airport\n9 ----> in\n2238 ----> dublin\n3 ----> .\n5 ----> <end>\n\nTarget Language; index to word mapping\n4 ----> <start>\n28 ----> dem\n2975 ----> lebensmittel\n7 ----> und\n11088 ----> veterinaramt\n750 ----> gehoren\n2858 ----> unabhangige\n2973 ----> wissenschaftler\n1 ----> ,\n19707 ----> veterinarmediziner\n1020 ----> usw\n3 ----> .\n45 ----> an\n1 ----> ,\n2 ----> die\n17 ----> fur\n89 ----> ihre\n19708 ----> reisetatigkeit\n20 ----> auf\n52 ----> einen\n5653 ----> flughafen\n2350 ----> angewiesen\n32 ----> sind\n1 ----> ,\n7 ----> und\n52 ----> einen\n5653 ----> flughafen\n35 ----> haben\n12 ----> wir\n8 ----> in\n3614 ----> dublin\n3 ----> .\n5 ----> <end>\n"
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "# BATCH_SIZE = 64\n",
    "## for custom model use\n",
    "BATCH_SIZE = 179\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(TensorShape([179, 179]), TensorShape([179, 179]))"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1,learning_rate=lr_schedule)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==============] - 0s 7ms/step - loss: 2.2823 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.2815 - accuracy: 1.8726e-04\n32\nTime taken to train batch: %s 12.859038829803467\nbatch loss: %f [2.1169726848602295, 0.0001092350430553779]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 1.9530 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9492 - accuracy: 9.3630e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 1.9478 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9489 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9499 - accuracy: 3.1210e-05\n33\nTime taken to train batch: %s 13.195049524307251\nbatch loss: %f [2.137209415435791, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.3232 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.3276 - accuracy: 9.3630e-05\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3229 - accuracy: 1.2484e-04\nEpoch 4/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.3270 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.3289 - accuracy: 3.1210e-05\n34\nTime taken to train batch: %s 13.55303406715393\nbatch loss: %f [2.310955762863159, 4.68150174128823e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.2973 - accuracy: 0.0000e+00\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.2989 - accuracy: 6.2420e-05\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.2973 - accuracy: 1.5605e-04\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2980 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3007 - accuracy: 3.1210e-05\n35\nTime taken to train batch: %s 13.878032922744751\nbatch loss: %f[2.237558364868164, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1680 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1709 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1701 - accuracy: 3.4331e-04\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1755 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1723 - accuracy: 6.2420e-05\n36\nTime taken to train batch: %s 14.231033086776733\nbatch loss: %f [2.297736406326294, 9.36300348257646e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.4263 - accuracy: 0.0000e+00\nEpoch 2/5\n6/6 [==============================] - 0s 10ms/step - loss: 2.4269 - accuracy: 6.2420e-05\nEpoch 3/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.4243 - accuracy: 9.3630e-05\nEpoch 4/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.4229 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.4229 - accuracy: 0.0000e+00\n37\nTime taken to train batch: %s 14.586035013198853\nbatch loss: %f [2.3388891220092773, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2547 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2548 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.2554 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.2573 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2579 - accuracy: 3.1210e-05\n38\nTime taken to train batch: %s 14.941032409667969\nbatch loss: %f [2.1664371490478516, 0.0001248400512849912]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.0770 - accuracy: 1.2484e-04\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0788 - accuracy: 1.2484e-04\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0764 - accuracy: 4.3694e-04\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0770 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0771 - accuracy: 9.3630e-05\n39\nTime taken to train batch: %s 15.281033039093018\nbatch loss: %f [2.089836597442627, 9.36300348257646e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1027 - accuracy: 0.0000e+00\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1016 - accuracy: 0.0000e+00\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1026 - accuracy: 0.0000e+00\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1036 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1019 - accuracy: 3.1210e-05\n40\nTime taken to train batch: %s 15.63003396987915\nbatch loss: %f [2.1271939277648926, 1.56050064106239e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1525 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1509 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1526 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1501 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1527 - accuracy: 9.3630e-05\n41\nTime taken to train batch: %s 15.999036073684692\nbatch loss: %f [2.0878000259399414, 0.0001092350430553779]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0214 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0217 - accuracy: 1.5605e-04\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0214 - accuracy: 4.0573e-04\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0211 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0222 - accuracy: 0.0000e+00\n42\nTime taken to train batch: %s 16.3500337600708\nbatch loss: %f [1.9606540203094482, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.8989 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9002 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 1.8994 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9016 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.8990 - accuracy: 9.3630e-05\n43\nTime taken to train batch: %s 16.677034616470337\nbatch loss: %f [2.0305705070495605, 4.68150174128823e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1604 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1592 - accuracy: 1.5605e-04\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1591 - accuracy: 1.5605e-04\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1591 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1585 - accuracy: 1.2484e-04\n44\nTime taken to train batch: %s 17.032033920288086\nbatch loss: %f [2.1421213150024414, 6.24200256424956e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1226 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.1233 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1227 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1225 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1220 - accuracy: 3.1210e-05\n45\nTime taken to train batch: %s 17.364035844802856\nbatch loss: %f [1.969422698020935, 9.36300348257646e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 1.8173 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.8188 - accuracy: 0.0000e+00\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.8188 - accuracy: 9.3630e-05\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.8169 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.8171 - accuracy: 3.1210e-05\n46\nTime taken to train batch: %s 17.684032917022705\nbatch loss: %f [1.909441590309143, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0017 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0014 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0011 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0010 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0010 - accuracy: 3.1210e-05\n47\nTime taken to train batch: %s 18.014036178588867\nbatch loss: %f [2.132903575897217, 7.80250338721089e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2655 - accuracy: 2.1847e-04\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.2659 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2709 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2718 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.2676 - accuracy: 1.5605e-04\n48\nTime taken to train batch: %s 18.371036052703857\nbatch loss: %f [2.154517889022827, 0.0001092350430553779]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0380 - accuracy: 1.2484e-04\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0380 - accuracy: 1.2484e-04\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0374 - accuracy: 9.3630e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0351 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0391 - accuracy: 1.2484e-04\n49\nTime taken to train batch: %s 18.72004747390747\nbatch loss: %f [2.0274269580841064, 9.36300348257646e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0161 - accuracy: 0.0000e+00\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0149 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0155 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0161 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0161 - accuracy: 4.3694e-04\n50\nTime taken to train batch: %s 19.083033561706543\nbatch loss: %f [2.0300865173339844, 0.0002340750943403691]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0442 - accuracy: 2.4968e-04\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0407 - accuracy: 0.0000e+00\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0405 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0398 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0407 - accuracy: 6.2420e-05\n51\nTime taken to train batch: %s 19.445034503936768\nbatch loss: %f [2.172081708908081, 6.24200256424956e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3053 - accuracy: 9.3630e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3085 - accuracy: 2.4968e-04\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3114 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.3050 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3079 - accuracy: 1.2484e-04\n52\nTime taken to train batch: %s 19.77304744720459\nbatch loss: %f [2.245483160018921, 0.0001092350430553779]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1787 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1761 - accuracy: 6.2420e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1771 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1737 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1757 - accuracy: 0.0000e+00\n53\nTime taken to train batch: %s 20.096033573150635\nbatch loss: %f [2.086611270904541, 0.0]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 10ms/step - loss: 1.9988 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 1.9990 - accuracy: 6.2420e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0009 - accuracy: 0.0000e+00\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9985 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 1.9973 - accuracy: 0.0000e+00\n54\nTime taken to train batch: %s 20.46036195755005\nbatch loss: %f [2.0080010890960693, 7.80250338721089e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0169 - accuracy: 0.0000e+00\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0178 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0153 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.0185 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.0183 - accuracy: 0.0000e+00\n55\nTime taken to train batch: %s 20.788362741470337\nbatch loss: %f [2.0827794075012207, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1502 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.1506 - accuracy: 2.4968e-04\nEpoch 3/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.1488 - accuracy: 9.3630e-05\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1477 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1473 - accuracy: 3.1210e-05\n56\nTime taken to train batch: %s 21.097371339797974\nbatch loss: %f [2.0775644779205322, 6.24200256424956e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0078 - accuracy: 2.4968e-04\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0083 - accuracy: 3.1210e-05\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0099 - accuracy: 1.2484e-04\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0094 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0109 - accuracy: 1.2484e-04\n57\nTime taken to train batch: %s 21.430373907089233\nbatch loss: %f [2.0860023498535156, 7.80250338721089e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1624 - accuracy: 3.1210e-05\nEpoch 2/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1607 - accuracy: 6.2420e-05\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1601 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 10ms/step - loss: 2.1625 - accuracy: 0.0000e+00\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.1599 - accuracy: 0.0000e+00\n58\nTime taken to train batch: %s 21.785370349884033\nbatch loss: %f [2.1345300674438477, 0.001778970705345273]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1107 - accuracy: 1.5605e-04\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1078 - accuracy: 6.2420e-05\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.1079 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1081 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.1077 - accuracy: 3.1210e-05\n59\nTime taken to train batch: %s 22.136372566223145\nbatch loss: %f [2.216242790222168, 3.12100128212478e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.3204 - accuracy: 1.8726e-04\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.3178 - accuracy: 1.5605e-04\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.3198 - accuracy: 2.4968e-04\nEpoch 4/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.3179 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.3174 - accuracy: 1.5605e-04\n60\nTime taken to train batch: %s 22.481895446777344\nbatch loss: %f [2.164125680923462, 7.80250338721089e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 10ms/step - loss: 2.0110 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0106 - accuracy: 9.3630e-05\nEpoch 3/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0112 - accuracy: 3.1210e-05\nEpoch 4/5\n6/6 [==============================] - 0s 6ms/step - loss: 2.0129 - accuracy: 6.2420e-05\nEpoch 5/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0125 - accuracy: 3.1210e-05\n61\nTime taken to train batch: %s 22.849897384643555\nbatch loss: %f [2.041661024093628, 6.24200256424956e-05]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0718 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0736 - accuracy: 7.4904e-04\nEpoch 3/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.0712 - accuracy: 6.2420e-05\nEpoch 4/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0707 - accuracy: 3.1210e-05\nEpoch 5/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.0698 - accuracy: 1.8726e-04\n62\nTime taken to train batch: %s 23.17989492416382\nbatch loss: %f [2.137874126434326, 0.0001248400512849912]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.2062 - accuracy: 6.2420e-05\nEpoch 2/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2068 - accuracy: 0.0000e+00\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 2.2076 - accuracy: 2.4968e-04\nEpoch 4/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.2065 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 7ms/step - loss: 2.2066 - accuracy: 1.2484e-04\n63\nTime taken to train batch: %s 23.517891883850098\nbatch loss: %f [2.1033318042755127, 0.0001092350430553779]\n(179, 179, 1)\nEpoch 1/5\n6/6 [==============================] - 0s 8ms/step - loss: 1.9988 - accuracy: 1.5605e-04\nEpoch 2/5\n6/6 [==============================] - 0s 7ms/step - loss: 1.9994 - accuracy: 1.2484e-04\nEpoch 3/5\n6/6 [==============================] - 0s 8ms/step - loss: 1.9988 - accuracy: 4.3694e-04\nEpoch 4/5\n6/6 [==============================] - 0s 6ms/step - loss: 1.9992 - accuracy: 9.3630e-05\nEpoch 5/5\n6/6 [==============================] - 0s 9ms/step - loss: 2.0004 - accuracy: 9.3630e-05\nINFO:tensorflow:Assets written to: ./models/nmt\\assets\n"
    }
   ],
   "source": [
    "def buildModel():\n",
    "    model = tf.keras.Sequential(layers=[\n",
    "        # tf.keras.layers.LSTM(64),\n",
    "        # tf.keras.layers.LSTM(16),\n",
    "#         tf.keras.layers.Conv1D(668,kernel_size=1),\n",
    "        tf.keras.layers.Conv1D(179,kernel_size=1),\n",
    "        tf.keras.layers.Conv1D(64,kernel_size=1),\n",
    "        tf.keras.layers.Conv1D(16,kernel_size=1),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "        # tf.keras.layers.Conv1D(64,kernel_size=1), #input_shape=(64,668)\n",
    "        # tf.keras.layers.MaxPool1D(),\n",
    "        # tf.keras.layers.Conv1D(1,kernel_size=1),\n",
    "        # tf.keras.layers.Dense(16),\n",
    "        # # # tf.keras.layers.MaxPool1D(),\n",
    "        # #\n",
    "        tf.keras.layers.Dense(16),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        # tf.keras.layers.Dense(179),\n",
    "        # tf.keras.layers.Dense(45342),\n",
    "        tf.keras.layers.Dense(668),\n",
    "        # tf.keras.layers.Dense(668),\n",
    "        # tf.keras.layers.Dense(4295821)\n",
    "\n",
    "        # tf.keras.layers.Conv1DTranspose(1,kernel_size=1),\n",
    "        # tf.keras.layers.Conv1DTranspose(668,kernel_size=1)\n",
    "\n",
    "        ])\n",
    "    return model\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# seq2seq.summary()\n",
    "\n",
    "# with tf.device('/cpu:0'): ## comment out to run on gpu\n",
    "#     start_time = time.time()\n",
    "#     seq2seq = buildModel()\n",
    "#     print(\"Time required to create model --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#     ## Train model and print results\n",
    "#     seq2seq.compile(optimizer= 'adam',loss=loss_function)\n",
    "#     seq2seq.build(input_shape=(668,64,1))\n",
    "#     # seq2seq.summary()\n",
    "#     start_time = time.time()\n",
    "#     for (batch, (inp, targ)) in enumerate(dataset.take(32)):\n",
    "#         inp = tf.reshape(inp,[inp.shape[0],inp.shape[1],1])\n",
    "#         targ = tf.reshape(targ,[targ.shape[0],targ.shape[1],1])\n",
    "#         seq2seq.train_on_batch(x=inp,y=targ)\n",
    "#         print(batch)\n",
    "#         print(\"Time taken to train batch: %s\", time.time()-start_time)\n",
    "#     #     print(targ.shape)\n",
    "#     #     seq2seq.fit(x=input_tensor_train,y=target_tensor_train,epochs=10)\n",
    "#     seq2seq.summary()\n",
    "#     seq2seq.save(filepath=\"./models/dense\")\n",
    "start_time = time.time()\n",
    "seq2seq = buildModel()\n",
    "print(\"Time required to create model --- %s seconds ---\" % (time.time() - start_time))\n",
    "## Train model and print results\n",
    "### Loss functions\n",
    "## MSE gives high error\n",
    "## SparseCategoricalCrossentropy gives nan\n",
    "## loss_function gives shape error\n",
    "## sparse_categorical_crossentropy give shape error\n",
    "print(\"y train\")\n",
    "print(target_tensor_train.shape)\n",
    "print(\"Y val\")\n",
    "print(target_tensor_val.shape)\n",
    "def loss_function(y_pred, y):\n",
    " \n",
    "    #shape of y [batch_size, ty]\n",
    "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
    "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                                  reduction='none')\n",
    "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "seq2seq.compile(optimizer=optimizer,loss='mean_squared_logarithmic_error',metrics= ['accuracy'])\n",
    "seq2seq.build(input_shape=(668,64,1))\n",
    "# seq2seq.summary()\n",
    "start_time = time.time()\n",
    "seq2seq.summary()\n",
    "for (batch, (inp, targ)) in enumerate(dataset.take(64)):\n",
    "    inp = tf.reshape(inp,[inp.shape[0],inp.shape[1],1])\n",
    "    targ = tf.reshape(targ,[targ.shape[0],targ.shape[1],1])\n",
    "    loss = seq2seq.train_on_batch(x=inp,y=targ)\n",
    "    print(batch)\n",
    "    print(\"Time taken to train batch: %s\", time.time()-start_time)\n",
    "    print(\"batch loss: %f\", loss)\n",
    "    print(targ.shape)\n",
    "    seq2seq.fit(x=inp,y=targ,epochs=5)\n",
    "seq2seq.save(filepath=\"./models/nmt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "ge_tokenizer.fit_on_texts(german_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['schones preises']\n[]\n['ausgewogene']\n['frage', 'ii']\n['zweiten']\n[]\n['abstimmen']\n['bildungs']\n[]\n['mir', 'demokratischen', 'wichtigsten', 'demokratischen']\n['luftschadstoffe']\n[]\n[]\n['mir']\n[]\n['mogliche', 'herren']\n"
    }
   ],
   "source": [
    "for (batch, (inp, targ)) in enumerate(dataset.take(16)):\n",
    "    # print(inp.shape)\n",
    "    # print(targ.shape)\n",
    "    inp = tf.reshape(inp,[inp.shape[0],inp.shape[1],1])\n",
    "    targ = tf.reshape(targ,[targ.shape[0],targ.shape[1],1])\n",
    "    predictions = []\n",
    "    predictions.append(seq2seq.predict_on_batch(inp))\n",
    "    german_output = ge_tokenizer.sequences_to_texts(predictions[0][batch-1])\n",
    "    # print(german_output.shape)\n",
    "    while(\"\" in german_output) : \n",
    "        german_output.remove('')\n",
    "    print(german_output)\n",
    "    # print(predictions)\n",
    "# print(predictions[0])\n",
    "results = predictions[0]\n",
    "# german_output = ge_tokenizer.sequences_to_texts(results)\n",
    "# print(results.shape)\n",
    "# print(results[1][1])\n",
    "# output = results[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def translate(sentence):\n",
    "#   inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "#   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "#                                                          maxlen=max_length_inp,\n",
    "\n",
    "#                                                          padding='post')\n",
    "#   inputs = tf.convert_to_tensor(inputs,dtype='float32')\n",
    "#   print(inputs.shape)\n",
    "#   inputs = input_tensor_val\n",
    "#   inputs = tf.reshape(inputs,[inputs.shape[0],inputs.shape[1],1])\n",
    "#   print(inputs.shape)\n",
    "  \n",
    "#   output=seq2seq.predict(x=inputs)\n",
    "\n",
    "#   # convert(targ_lang,output.all())\n",
    "# #   output=tf.convert_to_tensor(output,dtype='float32')\n",
    "#   # print(output[0].shape)\n",
    "# #   print(output.type)\n",
    "#   # print(output[0])\n",
    "#   x = output[0]\n",
    "#   print(x)\n",
    "\n",
    " \n",
    "# #   output_sample = output.tolist()\n",
    "#   ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "#       filters='')\n",
    "#   ge_tokenizer.fit_on_texts(german_data)\n",
    "#   german_output = ge_tokenizer.sequences_to_texts(x)\n",
    "#   print(german_output)\n",
    "# #   print(*(i for i in german_output),sep=\"\\n\")\n",
    "# translate(\"i declare resumed the session of the european parliament adjourned on friday december\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}